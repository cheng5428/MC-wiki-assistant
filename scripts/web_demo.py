import os

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["STREAMLIT_SERVER_WATCH_MODULES"] = "false"
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"

# æ›´ç›´æ¥çš„çŒ´å­è¡¥ä¸æ–¹æ³•
def apply_monkey_patch():
    try:
        from streamlit.watcher import local_sources_watcher
        
        # ä¿å­˜åŸå§‹å‡½æ•°
        original_get_module_paths = local_sources_watcher.get_module_paths
        
        # åˆ›å»ºä¸€ä¸ªå®‰å…¨çš„å‡½æ•°
        def safe_get_module_paths(module):
            try:
                # æ’é™¤torch.classesæ¨¡å—
                if hasattr(module, "__name__") and "torch.classes" in module.__name__:
                    return []
                return original_get_module_paths(module)
            except Exception as e:
                print(f"è·å–æ¨¡å—è·¯å¾„æ—¶å‡ºé”™: {e}")
                return []

        local_sources_watcher.get_module_paths = safe_get_module_paths
    except Exception as e:
        print(f"åº”ç”¨çŒ´å­è¡¥ä¸æ—¶å‡ºé”™: {e}")

apply_monkey_patch()

import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import streamlit as st
import random
import re
from threading import Thread
import base64
import torch

import numpy as np
from model.model_lora import *
from check_repetition import *

st.set_page_config(page_title="MiniMind", initial_sidebar_state="collapsed")

st.markdown("""
    <style>
        /* æ·»åŠ æ“ä½œæŒ‰é’®æ ·å¼ */
        .stButton button {
            border-radius: 50% !important;  /* æ”¹ä¸ºåœ†å½¢ */
            width: 32px !important;         /* å›ºå®šå®½åº¦ */
            height: 32px !important;        /* å›ºå®šé«˜åº¦ */
            padding: 0 !important;          /* ç§»é™¤å†…è¾¹è· */
            background-color: transparent !important;
            border: 1px solid #ddd !important;
            display: flex !important;
            align-items: center !important;
            justify-content: center !important;
            font-size: 14px !important;
            color: #666 !important;         /* æ›´æŸ”å’Œçš„é¢œè‰² */
            margin: 5px 10px 5px 0 !important;  /* è°ƒæ•´æŒ‰é’®é—´è· */
        }
        .stButton button:hover {
            border-color: #999 !important;
            color: #333 !important;
            background-color: #f5f5f5 !important;
        }
        .stMainBlockContainer > div:first-child {
            margin-top: -50px !important;
        }
        .stApp > div:last-child {
            margin-bottom: -35px !important;
        }
        
        /* é‡ç½®æŒ‰é’®åŸºç¡€æ ·å¼ */
        .stButton > button {
            all: unset !important;  /* é‡ç½®æ‰€æœ‰é»˜è®¤æ ·å¼ */
            box-sizing: border-box !important;
            border-radius: 50% !important;
            width: 18px !important;
            height: 18px !important;
            min-width: 18px !important;
            min-height: 18px !important;
            max-width: 18px !important;
            max-height: 18px !important;
            padding: 0 !important;
            background-color: transparent !important;
            border: 1px solid #ddd !important;
            display: flex !important;
            align-items: center !important;
            justify-content: center !important;
            font-size: 14px !important;
            color: #888 !important;
            cursor: pointer !important;
            transition: all 0.2s ease !important;
            margin: 0 2px !important;  /* è°ƒæ•´è¿™é‡Œçš„ margin å€¼ */
        }

    </style>
""", unsafe_allow_html=True)

system_prompt = []
device = "cuda" if torch.cuda.is_available() else "cpu"


def process_assistant_content(content):
    if model_source == "API" and 'R1' not in api_model_name:
        return content
    if model_source != "API" and 'R1' not in MODEL_PATHS[selected_model][1]:
        return content

    if '<think>' in content and '</think>' in content:
        content = re.sub(r'(<think>)(.*?)(</think>)',
                         r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\2</details>',
                         content,
                         flags=re.DOTALL)

    if '<think>' in content and '</think>' not in content:
        content = re.sub(r'<think>(.*?)$',
                         r'<details open style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†ä¸­...</summary>\1</details>',
                         content,
                         flags=re.DOTALL)

    if '<think>' not in content and '</think>' in content:
        content = re.sub(r'(.*?)</think>',
                         r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\1</details>',
                         content,
                         flags=re.DOTALL)

    return content


def load_model_tokenizer(model_path):
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    model = model.eval().to(device)
    return model, tokenizer


def clear_chat_messages():
    del st.session_state.messages
    del st.session_state.chat_messages


def init_chat_messages():
    if "messages" in st.session_state:
        for i, message in enumerate(st.session_state.messages):
            if message["role"] == "assistant":
                with st.chat_message("assistant", avatar=image_url):
                    st.markdown(process_assistant_content(message["content"]), unsafe_allow_html=True)
                    if st.button("ğŸ—‘", key=f"delete_{i}"):
                        st.session_state.messages.pop(i)
                        st.session_state.messages.pop(i - 1)
                        st.session_state.chat_messages.pop(i)
                        st.session_state.chat_messages.pop(i - 1)
                        st.rerun()
            else:
                st.markdown(
                    f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: #ddd; border-radius: 10px; color: black;">{message["content"]}</div></div>',
                    unsafe_allow_html=True)

    else:
        st.session_state.messages = []
        st.session_state.chat_messages = []

    return st.session_state.messages

def regenerate_answer(index):
    st.session_state.messages.pop()
    st.session_state.chat_messages.pop()
    st.rerun()


def delete_conversation(index):
    st.session_state.messages.pop(index)
    st.session_state.messages.pop(index - 1)
    st.session_state.chat_messages.pop(index)
    st.session_state.chat_messages.pop(index - 1)
    st.rerun()


st.sidebar.title("æ¨¡å‹è®¾å®šè°ƒæ•´")

# st.sidebar.text("è®­ç»ƒæ•°æ®åå·®ï¼Œå¢åŠ ä¸Šä¸‹æ–‡è®°å¿†æ—¶\nå¤šè½®å¯¹è¯ï¼ˆè¾ƒå•è½®ï¼‰å®¹æ˜“å‡ºç°èƒ½åŠ›è¡°å‡")
st.session_state.history_chat_num = st.sidebar.slider("Number of Historical Dialogues", 0, 6, 0, step=2)
# st.session_state.history_chat_num = 0
st.session_state.max_new_tokens = st.sidebar.slider("Max Sequence Length", 256, 8192, 8192, step=1)
st.session_state.temperature = st.sidebar.slider("Temperature", 0.6, 1.2, 0.85, step=0.01)

model_source = st.sidebar.radio("é€‰æ‹©æ¨¡å‹æ¥æº", ["æœ¬åœ°æ¨¡å‹", "API"], index=0)

if model_source == "API":
    api_url = st.sidebar.text_input("API URL", value="http://127.0.0.1:8000/v1")
    api_model_id = st.sidebar.text_input("Model ID", value="minimind")
    api_model_name = st.sidebar.text_input("Model Name", value="MiniMind2")
    api_key = st.sidebar.text_input("API Key", value="none", type="password")
    slogan = f"Hi, I'm {api_model_name}"
else:
    MODEL_PATHS = {
        "Minecraft ç»´åŸºç™¾ç§‘å°åŠ©æ‰‹": ["../MiniMind2-Small", "Minecraft ç»´åŸºç™¾ç§‘å°åŠ©æ‰‹", "../out/lora/lora_mc_40_512.pth", "../images/mc_wiki.png"],
        "MiniMind2-Small": ["../MiniMind2-Small", "MiniMind2-Small", "../images/logo2.png"]
    }

    selected_model = st.sidebar.selectbox('Models', list(MODEL_PATHS.keys()), index=0)  # é»˜è®¤é€‰æ‹© MCç»´åŸºç™¾ç§‘å°åŠ©æ‰‹
    lora_path = MODEL_PATHS[selected_model][2] if len(MODEL_PATHS[selected_model]) > 3 else None
    model_path = MODEL_PATHS[selected_model][0]
    slogan = f"Hi, I'm {MODEL_PATHS[selected_model][1]}"
    image_url = MODEL_PATHS[selected_model][-1]

def get_image_base64(image_path):
    """å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç """
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")
    except Exception as e:
        print(f"è¯»å–å›¾ç‰‡é”™è¯¯: {e}")
        return None


# head_image_urlç”¨äºé¡µé¢é¡¶éƒ¨ï¼Œä½¿ç”¨base64ç¼–ç 
local_head_image_path = image_url

if os.path.exists(local_head_image_path):
    # å°†æœ¬åœ°å›¾ç‰‡è½¬æ¢ä¸ºbase64
    img_base64 = get_image_base64(local_head_image_path)
    if img_base64:
        # æ ¹æ®å›¾ç‰‡ç±»å‹ç¡®å®šæ­£ç¡®çš„MIMEç±»å‹
        image_extension = os.path.splitext(local_head_image_path)[1].lower()
        mime_type = "image/jpeg" if image_extension in [".jpg", ".jpeg"] else "image/png" 
        head_image_url = f"data:{mime_type};base64,{img_base64}"
    else:
        # å¦‚æœæ— æ³•è¯»å–æœ¬åœ°å›¾ç‰‡ï¼Œä½¿ç”¨åœ¨çº¿å¤‡ä»½
        print("æ— æ³•è¯»å–æœ¬åœ°å›¾ç‰‡ï¼Œä½¿ç”¨åœ¨çº¿å¤‡ä»½")
        head_image_url = "https://www.modelscope.cn/api/v1/studio/gongjy/MiniMind/repo?Revision=master&FilePath=images%2Flogo2.png&View=true"
else:
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨åœ¨çº¿å¤‡ä»½
    print("æœ¬åœ°å›¾ç‰‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨åœ¨çº¿å¤‡ä»½")
    head_image_url = "https://www.modelscope.cn/api/v1/studio/gongjy/MiniMind/repo?Revision=master&FilePath=images%2Flogo2.png&View=true"


st.markdown(
    f'<div style="display: flex; flex-direction: column; align-items: center; text-align: center; margin: 0; padding: 0;">'
    '<div style="font-style: italic; font-weight: 900; margin: 0; padding-top: 4px; display: flex; align-items: center; justify-content: center; flex-wrap: wrap; width: 100%;">'
    f'<img src="{head_image_url}" style="width: 45px; height: 45px; "> '
    f'<span style="font-size: 26px; margin-left: 10px;">{slogan}</span>'
    '</div>'
    '<span style="color: #bbb; font-style: italic; margin-top: 6px; margin-bottom: 10px;">å†…å®¹å®Œå…¨ç”±AIç”Ÿæˆï¼Œè¯·åŠ¡å¿…ä»”ç»†ç”„åˆ«<br>Content AI-generated, please discern with care</span>'
    '</div>',
    unsafe_allow_html=True
)


def setup_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    if model_source == "æœ¬åœ°æ¨¡å‹":
        model, tokenizer = load_model_tokenizer(model_path)
        if lora_path and os.path.exists(lora_path):
            apply_lora(model)
            load_lora(model, lora_path)
    else:
        model, tokenizer = None, None

    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.chat_messages = []

    messages = st.session_state.messages

    for i, message in enumerate(messages):
        if message["role"] == "assistant":
            # ä½¿ç”¨æ¶ˆæ¯ä¸­å­˜å‚¨çš„å¤´åƒï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å½“å‰å¤´åƒ
            message_avatar = message.get("avatar", image_url)
            with st.chat_message("assistant", avatar=message_avatar):
                st.markdown(process_assistant_content(message["content"]), unsafe_allow_html=True)
                if st.button("Ã—", key=f"delete_{i}"):
                    st.session_state.messages = st.session_state.messages[:i - 1]
                    st.session_state.chat_messages = st.session_state.chat_messages[:i - 1]
                    st.rerun()
        else:
            st.markdown(
                f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: gray; border-radius: 10px; color:white; ">{message["content"]}</div></div>',
                unsafe_allow_html=True)

    prompt = st.chat_input(key="input", placeholder="ç»™ MiniMind å‘é€æ¶ˆæ¯")

    if hasattr(st.session_state, 'regenerate') and st.session_state.regenerate:
        prompt = st.session_state.last_user_message
        regenerate_index = st.session_state.regenerate_index
        delattr(st.session_state, 'regenerate')
        delattr(st.session_state, 'last_user_message')
        delattr(st.session_state, 'regenerate_index')

    if prompt:
        st.markdown(
            f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: gray; border-radius: 10px; color:white; ">{prompt}</div></div>',
            unsafe_allow_html=True)
        messages.append({"role": "user", "content": prompt[-st.session_state.max_new_tokens:]})
        st.session_state.chat_messages.append({"role": "user", "content": prompt[-st.session_state.max_new_tokens:]})

        with st.chat_message("assistant", avatar=image_url):
            placeholder = st.empty()


            if model_source == "API":
                try:
                    from openai import OpenAI

                    client = OpenAI(
                        api_key=api_key,
                        base_url=api_url
                    )
                    history_num = st.session_state.history_chat_num + 1  # +1 æ˜¯ä¸ºäº†åŒ…å«å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯
                    conversation_history = system_prompt + st.session_state.chat_messages[-history_num:]
                    answer = ""
                    response = client.chat.completions.create(
                        model=api_model_id,
                        messages=conversation_history,
                        stream=True,
                        temperature=st.session_state.temperature
                    )

                    for chunk in response:
                        content = chunk.choices[0].delta.content or ""
                        answer += content
                        placeholder.markdown(process_assistant_content(answer), unsafe_allow_html=True)

                except Exception as e:
                    answer = f"APIè°ƒç”¨å‡ºé”™: {str(e)}"
                    placeholder.markdown(answer, unsafe_allow_html=True)
            else:
                random_seed = random.randint(0, 2 ** 32 - 1)
                setup_seed(random_seed)

                st.session_state.chat_messages = system_prompt + st.session_state.chat_messages[
                                                                 -(st.session_state.history_chat_num + 1):]
                new_prompt = tokenizer.apply_chat_template(
                    st.session_state.chat_messages,
                    tokenize=False,
                    add_generation_prompt=True
                )

                inputs = tokenizer(
                    new_prompt,
                    return_tensors="pt",
                    truncation=True
                ).to(device)

                # streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
                streamer = RepetitionIteratorStreamer(tokenizer, skip_special_tokens=True)
                generation_kwargs = {
                    "input_ids": inputs.input_ids,
                    "max_length": inputs.input_ids.shape[1] + st.session_state.max_new_tokens,
                    "num_return_sequences": 1,
                    "do_sample": True,
                    "attention_mask": inputs.attention_mask,
                    "pad_token_id": tokenizer.pad_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                    "temperature": st.session_state.temperature,
                    "top_p": 0.85,
                    "streamer": streamer,
                }

                Thread(target=model.generate, kwargs=generation_kwargs).start()

                answer = ""
                for new_text in streamer:
                    answer += new_text
                    placeholder.markdown(process_assistant_content(answer), unsafe_allow_html=True)

            messages.append({
                "role": "assistant", 
                "content": answer, 
                "avatar": image_url  # ä¿å­˜å½“å‰ä½¿ç”¨çš„å¤´åƒ
            })
            st.session_state.chat_messages.append({
                "role": "assistant", 
                "content": answer,
                "avatar": image_url
            })
            with st.empty():
                if st.button("Ã—", key=f"delete_{len(messages) - 1}"):
                    st.session_state.messages = st.session_state.messages[:-2]
                    st.session_state.chat_messages = st.session_state.chat_messages[:-2]
                    st.rerun()


if __name__ == "__main__":
    from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

    main()
